# **Boris Hanin**

![image](https://user-images.githubusercontent.com/17192187/208241293-8cc211ec-8af6-446d-b4aa-e10e4b7581af.jpeg)
 
## **About Me**
I am an Assistant Professor at [Princeton ORFE](https://orfe.princeton.edu/) working on **deep learning, probability, and spectral asymptotics.** Prior to Princeton, I was an Assistant Professor in Mathematics at Texas A&M, an NSF Postdoc at MIT Math, and a PhD student in Math at Northwestern, where I was supervised by Steve Zelditch. 

I am also an advisor and member of the technical staff at [Foundry Technologies Inc.](https://www.mlfoundry.com/), an incredible AI/computing startup that seeks to orchestrate the world's compute. 

**Funding**: I am grateful to be supported by a 2024 Sloan Fellowship in Mathematics, NSF CAREER grant DMS-2143754 and NSF grants DMS-1855684, DMS-2133806. 

Please see my [CV](/CV.pdf) for more information.

**Email**: bhanin 'at' princeton.edu



## **News**
<!-- -  I am currently looking for grad students and postdocs. If you are a student at Princeton looking to work on deep learning theory, feel free to reach out (see also this somewhat tongue-in-cheek [writeup](/Working_With_Me.pdf)). -->
- Together with Gage DeZoort, Mariangela Lisanti, Dan Marlow, and Peter Elmer, I am co-organizing at new seminar on Machine Learning in Physics. Here is the [website](https://ml.physics.princeton.edu). 
-  Together with Jacob Foster (Sociology, UCLA), Jessica Flack (Collective Computation Group, Santa Fe Institute), Josh Tenenbaum (Brain and Cognitive Science, MIT), Max Kleiman-Weiner (Common Sense Machines), Orit Peleg (Computer Science, Colorado), Patrick Shafto (Rutgers/IAS, Mathematics), Pranab Das (Physics, Elon), Tom Griffiths (AI & Cognitive Science, Princeton) I am organizing a semester-long program at IPAM in Fall 2024 on the Mathematics of Intelligences. This program will bring together researchers working on learning in both biological and artifical settings with the goal of identifying and making progress on mathemcial questions related to learning and intelligence. Here is the [website](http://www.ipam.ucla.edu/programs/long-programs/mathematics-of-intelligences-2/).
<!-- -  I am speaking at a two-day program in mid-August on the theoretical aspects of Machine Learning at the Center for Brains, Minds, and Machines [Summer School](https://cbmm.mit.edu/summer-school) in Woods Hole, MA. -->
<!-- -  I am speaking at a [workshop](https://cargese2023.github.io) on statistical physics and machine learning in beautiful Cargese, Corsica, which runs July 31 - August 12. -->
<!-- -  I will be a plenary speaker at the 2023 IAIFI Summer Workshop on Theoretical Physics and ML in August 2023. This [workshop](https://iaifi.org/summer-workshop.html) will immediately follow the 2023 IAIFI Summer School. -->
-  I am organizing in Summer 2024 another installment of the Princeton Machine Learning Theory Summer School [website](https://mlschool.princeton.edu).
<!-- -  I am giving a mini-course on wide neural networks at the Rome Center on Mathematics for Modeling and Data Sciences. Here are some [notes](/TV_Lectures.pdf). -->


## **Research Group**
- I am fortunate to supervise several excellent PhD students: [Pierfrancesco Beneventano](https://pierbeneventano.github.io/)  and [Kaiqi Jiang](https://ece.princeton.edu/people/kaiqi-jiang).

- I am excited to be working with two phenomenal postdocs: [Gage DeZoort](https://gagedezoort.github.io) and [Mufan Li](https://mufan-li.github.io).

### **First Placement of Former Members**

- [Samy Jelassi](https://sjelassi.github.io) (Postdoc at Harvard CMSA)

## Professional Service
I am an Associate Editor of

- [Pure and Applied Analysis](https://msp.org/paa/about/journal/about.html)
- [Mathematics of Operations Research](https://pubsonline.informs.org/journal/moor)
- [Advances in Theoretical and Mathematical Physics](https://www.intlpress.com/site/pub/pages/journals/items/atmp/_home/_main/index.php)

All these journals are always looking for high quality submissions on theoretical machine learning. 


## Short Courses

1. Mathematics Aspects of Deep Learning Theory. University of Luxembourg. June 2024. 
2. Neural Networks and Gaussian Processes. Tor Vergata (Rome). Jan 2023. [notes](http://www.mat.uniroma2.it/~rds/Slides/Hanin.pdf); Lecture 1 [video](https://www.youtube.com/watch?v=k3hVE_rx0mE&t), Lecture 2 [video](https://www.youtube.com/watch?v=qANuMTwvsx8), Lecture 3 [video](https://www.youtube.com/watch?v=xQ6fpaDDD-g)
3. Neural Networks at Large and Infinite Width (joint with Yasaman Bahri). Les Houches Summer School on Statistical Physics of Machine Learning (France). July 2022. Lecture 1 [video](https://www.youtube.com/watch?v=mEdUi8gu3vE&list=PLEIq5bchE3R1QYiNthdj9rJDa4TUzR-Yb&index=4), Lecture 2 [video](https://www.youtube.com/watch?v=o4tDhCJTu0M&list=PLEIq5bchE3R1QYiNthdj9rJDa4TUzR-Yb&index=5), Lecture Notes [arXiv](https://arxiv.org/abs/2309.01592)
   

## **Papers** [ArXiv](https://arxiv.org/a/hanin_b_1.html)

### **Deep Learning**

#### **Preprints**
1. Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems, with L. Chen, J. Q. Davis, P. Bailis, M. Zaharia, I. Stoica, and J. Zou (2024) [ArXiv](https://arxiv.org/abs/2403.02419)
1. Quantitative CLTs in Deep Neural Networks, with S. Favaro, D. Marinucci, I. Nourdin, and G. Pecatti  (2023) [ArXiv](https://arxiv.org/abs/2307.06092)
1. Principles for Initialization and Architecture Selection in Graph Neural Networks with ReLU Activations, with G. DeZoort  (2023) [ArXiv](https://arxiv.org/abs/2306.11668)
1. Depth Dependence of μP Learning Rates in ReLU MLPs, with S. Jelassi, Z. Ji, S. Reddi, S. Bhojanapalli, and S. Kumar (2023) [ArXiv](https://arxiv.org/abs/2305.07810)
1. Random Fully Connected Neural Networks as Perturbatively Solvable Hierarchies (2022)  [ArXiv](https://arxiv.org/abs/2204.01058)
1. Ridgeless Interpolation with Shallow ReLU Networks in 1D is Nearest Neighbor Curvature Extrapolation and Provably Generalizes on Lipschitz Functions (2021)  [ArXiv](https://arxiv.org/abs/2109.12960)
2.	Approximating Continuous Functions by ReLU Nets of Minimal Width, with M. Sellke (2017) [ArXiv](https://arxiv.org/abs/1710.11278)


#### **Journal Articles**
1. Bayesian Interpolation with Deep Linear Networks, with A. Zlokapa, PNAS (2023) [ArXiv](https://arxiv.org/abs/2212.14457)
8. Random Neural Networks in the Infinite Width Limit as Gaussian Processes, Annals of Applied Probability, 2023, Vol. 33, No. 6A, 4798 – 4819 [ArXiv](https://arxiv.org/abs/2107.01562)
9. Non-asymptotic Results for Singular Values of Gaussian Matrix Products, with G. Paouris. GAFA (2021) [ArXiv](https://arxiv.org/abs/2005.08899)
14. Products of Many Large Random Matrices and Gradients in Deep Neural Networks, with M. Nica. Communications in Mathematical Physics (2020) [ArXiv](https://arxiv.org/abs/1812.05994)
1. Neural Network Approximation, with R. DeVore and G. Petrova, Acta Numerica (2020) [ArXiv](https://arxiv.org/abs/2012.14501)
17.	Nonlinear Approximation and (Deep) ReLU Networks, with I. Daubechies, R. DeVore, S. Foucart, and G. Petrova. Constructive Approximation (Special Issue on Deep Networks in Approximation Theory) (2019) [ArXiv](https://arxiv.org/abs/1905.02199)
22.	Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations. Mathematics 2019, 7(10), 992 (Special Issue on Computational Mathematics, Algorithms, and Data Processing) [ArXiv](https://arxiv.org/abs/1708.02691)

#### **Conference Articles**
1. Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit, with B. Bordelon, L. Noci, M. Li, and C. Pehlevan. ICLR 2024. [ArXiv](https://arxiv.org/abs/2309.16620)
2. Principled Architecture-Aware Scaling of Hyperparameters, with W. Chen, J. Wu, and Z. Wang. ICLR 2024. [Arxiv](https://arxiv.org/abs/2402.17440)
1. Maximal Initial Learning Rates in Deep ReLU Networks, with G. Iyer and D. Rolnick, ICML 2023 [ArXiv](https://arxiv.org/abs/2212.07295)
5. Deep Architecture Connectivity Matters for Its Convergence: A Fine-Grained Analysis with W. Chen, W. Huang, X. Gong, Z. Wang, NeurIPS 2022 [ArXiv](https://arxiv.org/abs/2205.05662)
1.	Finite Depth and Width Corrections to the Neural Tangent Kernel, with M. Nica, Splotlight at ICLR 2020 [ArXiv](https://arxiv.org/abs/1909.05989)
10. Deep ReLU Networks Preserve Expected Length, with R. Jeong and D. Rolnick, ICLR 2022 [ArXiv](https://arxiv.org/abs/2102.10492)
12. How Data Augmentation affects Optimization for Linear Regression, with Y. Sun NeurIPS 2021 [ArXiv](https://arxiv.org/abs/2010.11171)
1. Deep ReLU Networks Have Surprisingly Few Activation Patterns, with D. Rolnick, NeurIPS 2019 [ArXiv](https://arxiv.org/abs/1906.00904)
18.	Complexity of Linear Regions in Deep Networks, with D. Rolnick, ICML 2019 [ArXiv](https://arxiv.org/abs/1901.09021)
20.	Which Neural Net Architectures Give Rise to Vanishing and Exploding Gradients? NIPS 2018 [ArXiv](https://arxiv.org/abs/1801.03744)
13. How to Start Training: The Effect of Initialization and Architecture, with D. Rolnick. NIPS 2018 [ArXiv](https://arxiv.org/abs/1803.01719)




### **Spectral Theory**
##### **Journal Articles**

1. Scaling Asymptotics of Spectral Wigner Functions, with S. Zelditch. Journal of Physics A (Special Edition on Claritons and the Asymptotics of Ideas: the Physics of Michael Berry) (2022) [ArXiv](https://arxiv.org/abs/2207.13571)
13. Interface Asymptotics of Wigner-Weyl Distributions for the Harmonic Oscillator, with S. Zelditch. Journal d'Analyse (2022) [ArXiv](https://arxiv.org/abs/1903.12524)
15.	Interface Asymptotics of Eigenspace Wigner distributions for the Harmonic Oscillator, with S. Zelditch. Communications in PDE (2020) [ArXiv](https://arxiv.org/abs/1901.06438)
22.	Level Spacings and Nodal Sets at Infinity for Radial Perturbations of the Harmonic Oscillator, with T. Beck. International Math Research Notices, 2021. [ArXiv](https://arxiv.org/abs/1708.06434)
23.	Local Universality for Zeros and Critical Points of Monochromatic Random Waves, with Y. Canzani. Communication in Mathematical Physics, 2020. [ArXiv](https://arxiv.org/abs/1610.09438)
24.	Nodal Sets of Functions with Finite Vanishing Order, with T. Beck and S. Becker-Khan. Calculus of Variations and PDE (2018) [ArXiv](https://arxiv.org/abs/1708.06434)
25.	Scaling of Harmonic Oscillator Eigenfunctions and Their Nodal Sets Around the Caustic, with S. Zelditch and P. Zhou. Communications in Mathematical Physics. Vol. 350, no. 3, pp. 1147--1183, 2017. [ArXiv](https://arxiv.org/abs/1708.06434)
26. C^∞ Scaling Asymptotics for the Spectral Function of the Laplacian, with Y. Canzani. The Journal of Geometric Analysis (2018) [ArXiv](http://arxiv.org/abs/1602.00730)
28. Scaling Limit for the Kernel of the Spectral Projector and Remainder Estimates in the Pointwise Weyl Law, with Y. Canzani. Analysis and PDE, Vol. 8 (2015), No. 7, pp. 1707-1731. [ArXiv](http://arxiv.org/abs/1411.0658)
29. High Frequency Eigenfunction Immersions and Supremum Norms of Random Waves, with Y. Canzani. Electronic Research Announcements. MS 22, no. 0, January 2015, pp. 76 - 86. [ArXiv](http://arxiv.org/abs/1406.2309)
31. Nodal Sets of Random Eigenfunctions for the Isotropic Harmonic Oscillator, with S. Zelditch and P. Zhou. International Mathematics Research Notices, Vol. 2015, No. 13, pp. 4813 - 4839. [ArXiv](http://arxiv.org/abs/1310.4532)


### **Zeros and Critical Points of Random Polynomials**
##### **Journal Articles**
1. The Lemniscate Tree of a Random Polynomial, with M. Epstein and E. Lundberg. Annales Institute Henri Poincare (B), 2018. [ArXiv](https://arxiv.org/abs/1806.00521)
27.	Pairing of Zeros and Critical Points for Random Polynomials. Annales de l'Institut Henri Poincare (B) Probabilites et Statistiques. Volume 53, Number 3 (2017), 1498-1511. [ArXiv](https://arxiv.org/abs/1601.06417)
30. Pairing of Zeros and Critical Points for Random Meromorphic Functions on Riemann Surfaces</b>. Mathematics Research Letters, Vol. 22 (2015), No. 1, pp. 111-140. [ArXiv](http://arxiv.org/abs/1305.6105)
32. Correlations and Pairing Between Zeros and Critical Points of Gaussian Random Polynomials. International Math Research Notices (2015), Vol. (2), pp. 381-421. [ArXiv](http://arxiv.org/abs/1207.4734)

### **Other**
1. Contributed research to Principles of Deep Learning Theory, written by D. Roberts and S. Yaida, Cambridge University Press (2021) [ArXiv](https://arxiv.org/abs/2106.10165)
34. An Intriguing Property of the Center of Mass for Points on Quadradtic Curves and Surfaces, with L. Hanin and R. Fisher. Mathematics Maganize, v. 80, No. 5, pp. 353-362, 2007.
